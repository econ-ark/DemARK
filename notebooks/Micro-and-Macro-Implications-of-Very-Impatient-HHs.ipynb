{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micro- and Macroeconomic Implications of Very Impatient Households\n",
    "\n",
    "<p style=\"text-align: center;\"><small><small><small>For the following badges: GitHub does not allow click-through redirects; right-click to get the link, then paste into navigation bar</small></small></small></p>\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/econ-ark/DemARK/master?filepath=notebooks%2FMicro-and-Macro-Implications-of-Very-Impatient-HHs.ipynb)\n",
    "\n",
    "[![Open in CoLab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/econ-ark/DemARK/blob/master/notebooks/Micro-and-Macro-Implications-of-Very-Impatient-HHs.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Buffer stock saving models of the kind implemented in $\\texttt{ConsIndShockModel}$ say that, if a standard ['Growth Impatience Condition'](https://econ.jhu.edu/people/ccarroll/papers/BufferStockTheory/#Growth-Modified-Conditions) holds:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\newcommand{\\Rfree}{\\mathsf{R}}\\newcommand{\\DiscFac}{\\beta}\\newcommand{\\PermGroFac}{\\Gamma}\\newcommand{\\PermShk}{\\psi}\\newcommand{\\CRRA}{\\rho}\n",
    "\\left(\\frac{(\\Rfree\\DiscFac)^{1/\\CRRA}\\mathbb{E}[\\PermShk^{-1}]}{\\PermGroFac}\\right) & < & 1\n",
    "\\end{eqnarray}\n",
    "\n",
    "then the _ratio_ of asets $\\newcommand{\\aLev}{\\mathbf{a}}\\aLev$ to permanent income $\\newcommand{\\pLev}{\\mathbf{p}}\\pLev$, $a=\\aLev/\\pLev$, has a target value $\\newcommand{\\aTarg}{\\check{a}}\\aTarg$ that depends on the consumer's preferences (relative risk aversion $\\CRRA$, time preference $\\DiscFac$) and circumstances (interest factor $\\Rfree$, growth factor $\\PermGroFac$, uncertainty about permanent income shocks $\\sigma^{2}_{\\PermShk}$).\n",
    "\n",
    "If everyone had identical preferences and everyone were at their target $\\check{a}$, then inequality in the level of $\\aLev$ would be exactly the same as inequality in $\\pLev$.\n",
    "\n",
    "[\"The Distribution of Wealth and the Marginal Propensity to Consume\"](http://econ.jhu.edu/people/ccarroll/papers/cstwMPC) (Carroll, Slacalek, Tokuoka, and White 2017; hereafter: \"cstwMPC\") shows that, when such a model is simulated and agents draw their idiosyncratic shocks (so, agents are _ex post_ heterogeneous -- see the definition in [Intro-To-HARK](http://github.com/econ-ark/PARK/tree/master/Intro-To-HARK.pdf)) -- asset inequality is indeed close to $\\pLev$ inequality even though everyone is not always at exactly their target $a$.\n",
    "\n",
    "But a large body of evidence shows that _actual_ inequality in assets is much greater than _actual_ inequality in permanent income.  Thus, to make a model that qualifies as what cstwMPC call a 'serious' microfounded macro model of consumption (one that matches the key facts that _theory says_ should be first-order important), the model must be modified to incorporate some form of _ex ante_ heterogeneity: That is, there must be differences across people in $\\DiscFac$ or $\\Rfree$ or $\\CRRA$ or $\\PermGroFac$ or $\\sigma^{2}_{\\PermShk}$.\n",
    "\n",
    "The most transparent and simplest of these to change is the time preference factor $\\beta$.  So that is what the paper does.  The main results are:\n",
    "\n",
    "1. The distribution of $\\DiscFac$ need not be particularly wide to match the extreme concentration of wealth: roughly 0.91 to 0.98 (annual); that is, the most impatient person discounts the future about 6 percentage points more per year than the most patient agent agent\n",
    "2. With such a distribution of $\\DiscFac$, simulated agents' (annual) marginal propensity to consume (MPC) from transitory income shocks to income matches large body of microeconomic evidence that typically finds evidence of MPC's in the range of 0.2 to 0.6.  This is much better than RA macro models that typically yield MPC's in the range of 0.01 to 0.05.\n",
    "\n",
    "While the most impatient agents in the cstwMPC model have fairly high MPCs (~0.6 annual), there is microeconomic evidence that a significant fraction of households have *even higher* MPCs than the model predicts, especially at the quarterly frequency.  This group of households is commonly referred to as \"hand-to-mouth\" -- they consume most of their transitory shocks to income not long after they receive them (mostly within a quarter).  There are several reasons why a household could be hand-to-mouth, but one plausible explanation is that these households are *even more impatient* than estimated by cstwMPC for the most impatient agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     25
    ]
   },
   "outputs": [],
   "source": [
    "# This cell does some setup and imports generic tools used to produce the figures\n",
    "\n",
    "import sys \n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('../lib'))\n",
    "\n",
    "from tdqm import tqdm\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "import HARK # Prevents import error from Demos repo\n",
    "from HARK.utilities import plotFuncs\n",
    "\n",
    "\n",
    "Generator=False # Is this notebook the master or is it generated?\n",
    "# Import related generic python packages\n",
    "\n",
    "# Set how many digits past the decimal point should be printed?\n",
    "from time import clock\n",
    "mystr   = lambda number : \"{:.4f}\".format(number)\n",
    "decfmt4 = lambda number : \"{:.4f}\".format(number)\n",
    "decfmt3 = lambda number : \"{:.3f}\".format(number)\n",
    "decfmt2 = lambda number : \"{:.2f}\".format(number)\n",
    "decfmt1 = lambda number : \"{:.1f}\".format(number)\n",
    "\n",
    "# This is a jupytext paired notebook that autogenerates BufferStockTheory.py\n",
    "# which can be executed from a terminal command line via \"ipython BufferStockTheory.py\"\n",
    "# But a terminal does not permit inline figures, so we need to test jupyter vs terminal\n",
    "# Google \"how can I check if code is executed in the ipython notebook\"\n",
    "\n",
    "from IPython import get_ipython # In case it was run from python instead of ipython\n",
    "def in_ipynb():\n",
    "    try:\n",
    "        if str(type(get_ipython())) == \"<class 'ipykernel.zmqshell.ZMQInteractiveShell'>\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except NameError:\n",
    "        return False\n",
    "\n",
    "# Determine whether to make the figures inline (for spyder or jupyter)\n",
    "# vs whatever is the automatic setting that will apply if run from the terminal\n",
    "if in_ipynb():\n",
    "    # %matplotlib inline generates a syntax error when run from the shell\n",
    "    # so do this instead\n",
    "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "else:\n",
    "    get_ipython().run_line_magic('matplotlib', 'auto')\n",
    "\n",
    "# Import the plot-figure library matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# In order to use LaTeX to manage all text layout in our figures, we import rc settings from matplotlib.\n",
    "from matplotlib import rc\n",
    "plt.rc('font', family='serif')\n",
    "\n",
    "# LaTeX is huge and takes forever to install on mybinder\n",
    "# so if it is not installed then do not use it \n",
    "from distutils.spawn import find_executable\n",
    "iflatexExists=False\n",
    "if find_executable('latex'):\n",
    "    iflatexExists=True\n",
    "    \n",
    "plt.rc('text', usetex= iflatexExists)\n",
    "\n",
    "# The warnings package allows us to ignore some harmless but alarming warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from copy import copy, deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrating a Basic Version of cstwMPC\n",
    "\n",
    "To get started, let's reproduce a simplified version of the main results from cstwMPC.  \n",
    "\n",
    "In cstwMPC, the authors calibrated nearly all of the model parameters-- risk aversion, income shock process, etc-- to commonly used or previously estimated values.  The only parameter to be estimated is the distribution of $\\beta$.  cstwMPC assumed that $\\beta$ is uniformly distributed on $[\\grave{\\beta}-\\nabla,\\grave{\\beta}+\\nabla]$, approximated by a seven point distribution.\n",
    "\n",
    "Their estimation procedure seeks the values of $\\grave{\\beta}$ and $\\nabla$ that generate a simulated distribution of wealth that best matches empirical U.S. data.  Their definition of \"best match\" has two aspects:\n",
    "\n",
    "1. The simulated aggregate capital-to-income ratio matches the true U.S. value.\n",
    "2. The sum of squared distances between the simulated and empirical Lorenz curves (at the 20th, 40th, 60th, and 80th percentiles) is minimized (conditional on item 1).\n",
    "\n",
    "cstwMPC's target empirical moments are a capital-to-income ratio of 10.26 and cumulative wealth shares as given in the table below.  Yes, you are reading the table correctly: The \"poorest\" 80 percent of households own 17.5 percent of wealth. \n",
    "\n",
    "| Net worth percentile | Cumulative wealth share |\n",
    "|:---:|:---:|\n",
    "|  20th  | -0.2% |\n",
    "|  40th  | 1.0% |\n",
    "|  60th  | 5.5% |\n",
    "|  80th  | 17.5% |\n",
    "\n",
    "To reproduce their basic results, we must import an $\\texttt{AgentType}$ subclass and define a dictionary with calibrated parameters identical to those in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Import IndShockConsumerType\n",
    "from HARK.ConsumptionSaving.ConsIndShockModel import IndShockConsumerType\n",
    "\n",
    "# Define a dictionary with calibrated parameters\n",
    "cstwMPC_calibrated_parameters = {\n",
    "    \"CRRA\":1.0,                    # Coefficient of relative risk aversion \n",
    "    \"Rfree\":1.01/(1.0 - 1.0/160.0), # Survival probability,\n",
    "    \"PermGroFac\":[1.000**0.25], # Permanent income growth factor (no perm growth),\n",
    "    \"PermGroFacAgg\":1.0,\n",
    "    \"BoroCnstArt\":0.0,\n",
    "    \"CubicBool\":False,\n",
    "    \"vFuncBool\":False,\n",
    "    \"PermShkStd\":[(0.01*4/11)**0.5],  # Standard deviation of permanent shocks to income\n",
    "    \"PermShkCount\":5,  # Number of points in permanent income shock grid\n",
    "    \"TranShkStd\":[(0.01*4)**0.5],  # Standard deviation of transitory shocks to income,\n",
    "    \"TranShkCount\":5,  # Number of points in transitory income shock grid\n",
    "    \"UnempPrb\":0.07,  # Probability of unemployment while working\n",
    "    \"IncUnemp\":0.15,  # Unemployment benefit replacement rate\n",
    "    \"UnempPrbRet\":None,\n",
    "    \"IncUnempRet\":None,\n",
    "    \"aXtraMin\":0.00001,  # Minimum end-of-period assets in grid\n",
    "    \"aXtraMax\":40,  # Maximum end-of-period assets in grid\n",
    "    \"aXtraCount\":32,  # Number of points in assets grid\n",
    "    \"aXtraExtra\":[None],\n",
    "    \"aXtraNestFac\":3,  # Number of times to 'exponentially nest' when constructing assets grid\n",
    "    \"LivPrb\":[1.0 - 1.0/160.0],  # Survival probability\n",
    "    \"DiscFac\":0.97,             # Default intertemporal discount factor; dummy value, will be overwritten\n",
    "    \"cycles\":0,\n",
    "    \"T_cycle\":1,\n",
    "    \"T_retire\":0,\n",
    "    'T_sim':1200,  # Number of periods to simulate (idiosyncratic shocks model, perpetual youth)\n",
    "    'T_age': 400,\n",
    "    'IndL': 10.0/9.0,  # Labor supply per individual (constant),\n",
    "    'aNrmInitMean':np.log(0.00001),\n",
    "    'aNrmInitStd':0.0,\n",
    "    'pLvlInitMean':0.0,\n",
    "    'pLvlInitStd':0.0,\n",
    "    'AgentCount':10000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make several instances of our class of agents and give them different values of $\\beta$, following cstwMPC's estimated distribution.  In our specification of interest, we will use $\\grave{\\beta}=0.9855583$ and $\\nabla = 0.0085$.\n",
    "\n",
    "NB: Reported parameter estimates in cstwMPC use a model with aggregate shocks and wage and interest rates determined dynamically (a heterogeneous agents DSGE model); this is the $\\texttt{AggShockConsumerType}$ in HARK.  The estimated parameters are slightly different in this exercise, as we are ignoring general equilibrium aspects and only using the $\\texttt{IndShockConsumerType}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell constructs seven instances of IndShockConsumerType with different discount factors\n",
    "from HARK.utilities import approxUniform\n",
    "BaselineType = IndShockConsumerType(**cstwMPC_calibrated_parameters)\n",
    "\n",
    "# Specify the distribution of the discount factor\n",
    "num_types = 7              # number of types we want\n",
    "DiscFac_mean   = 0.9855583 # center of beta distribution \n",
    "DiscFac_spread = 0.0085    # spread of beta distribution\n",
    "DiscFac_dstn = approxUniform(num_types, DiscFac_mean-DiscFac_spread, DiscFac_mean+DiscFac_spread)[1]\n",
    "\n",
    "MyTypes = [] # initialize an empty list to hold our consumer types\n",
    "for nn in range(num_types):\n",
    "    # Now create the types, and append them to the list MyTypes\n",
    "    NewType = deepcopy(BaselineType)\n",
    "    NewType.DiscFac = DiscFac_dstn[nn]\n",
    "    NewType.seed = nn # give each consumer type a different RNG seed\n",
    "    MyTypes.append(NewType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving and Simulating the Baseline Agents\n",
    "\n",
    "Now let's solve and simulate each of our types of agents.  If you look in the parameter dictionary (or at any of the agent objects themselves), you will see that each one has an $\\texttt{AgentCount}$ attribute of 10000. That is, these seven ex ante heterogeneous types each represent ten thousand individual agents that will experience ex post heterogeneity when they draw different income (and mortality) shocks over time.\n",
    "\n",
    "In the code block below, fill in the contents of the loop to solve and simulate each agent type for many periods.  To do this, you should invoke the methods $\\texttt{solve}$, $\\texttt{initializeSim}$, and $\\texttt{simulate}$ in that order.  Simulating for 1200 quarters (300 years) will approximate the long run distribution of wealth in the population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress bar keeps track interactively of how many have been made\n",
    "for ThisType in tdqm(MyTypes):\n",
    "    ThisType.solve()\n",
    "    ThisType.initializeSim()\n",
    "    ThisType.simulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that you wrote that code correctly, let's check that the aggregate level of capital (total assets held by all households) to income ratio equals what we expected it would be.  To do that, let's combine the asset holdings of all types, take the mean, and see if we get the desired capital to income ratio of 10.26.\n",
    "\n",
    "NB: Because there is no permanent income growth in this model, all shocks are mean one and idiosyncratic, and we have many agents, aggregate or average income is 1.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of aggregate capital to permanent income is 10.26\n"
     ]
    }
   ],
   "source": [
    "aLvl_all = np.concatenate([ThisType.aLvlNow for ThisType in MyTypes])\n",
    "print('The ratio of aggregate capital to permanent income is ' + decfmt2(np.mean(aLvl_all)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Lorenz Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Volumes/Sync/Sys/OSX/linked/root/usr/local/bin/anaconda/lib/python3.6/site-packages/HARK/cstwMPC/USactuarial.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-20e277edf0ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot Lorenz curves for model with uniform distribution of time preference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mHARK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcstwMPC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetupParamsCSTW\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSCF_wealth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSCF_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mHARK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetLorenzShares\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetPercentiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpctiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.999\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/Sync/Sys/OSX/linked/root/usr/local/bin/anaconda/lib/python3.6/site-packages/HARK/cstwMPC/SetupParamsCSTW.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;31m# Import survival probabilities from SSA data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0mdata_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_location\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'USactuarial.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0mactuarial_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0mraw_actuarial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactuarial_reader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Volumes/Sync/Sys/OSX/linked/root/usr/local/bin/anaconda/lib/python3.6/site-packages/HARK/cstwMPC/USactuarial.txt'"
     ]
    }
   ],
   "source": [
    "# Plot Lorenz curves for model with uniform distribution of time preference\n",
    "from HARK.cstwMPC.SetupParamsCSTW import SCF_wealth, SCF_weights\n",
    "from HARK.utilities import getLorenzShares, getPercentiles\n",
    "\n",
    "pctiles = np.linspace(0.001,0.999,200)\n",
    "sim_wealth = np.concatenate([ThisType.aLvlNow for ThisType in MyTypes])\n",
    "SCF_Lorenz_points = getLorenzShares(SCF_wealth,weights=SCF_weights,percentiles=pctiles)\n",
    "sim_Lorenz_points = getLorenzShares(sim_wealth,percentiles=pctiles)\n",
    "plt.plot(pctiles,SCF_Lorenz_points,'--k')\n",
    "plt.plot(pctiles,sim_Lorenz_points,'-b')\n",
    "plt.xlabel('Percentile of net worth')\n",
    "plt.ylabel('Cumulative share of wealth')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Lorenz Distance at Targets\n",
    "\n",
    "Now we want to construct a function that calculates the Euclidean distance between simulated and actual Lorenz curves at the four percentiles of interest: 20, 40, 60, and 80.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Distribution Of the Marginal Propensity to Consume\n",
    "\n",
    "For many macroeconomic purposes, the distribution of the MPC $\\kappa$ is more important than the distribution of wealth.  Ours is a quarterly model, and MPC's are typically reported on an annual basis; we can compute an approximate MPC from the quraterly ones as $\\kappa_{Y} \\approx 1.0 - (1.0 - \\kappa_{Q})^4$\n",
    "\n",
    "In the cell below, we retrieve the MPCs from our simulated consumers and show that the 10th percentile in the MPC distribution is only about 6 percent, while at the 90th percentile it is almost 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the MPC's\n",
    "percentiles=np.linspace(0.1,0.9,9)\n",
    "MPC_sim = np.concatenate([ThisType.MPCnow for ThisType in MyTypes])\n",
    "MPCpercentiles_quarterly = getPercentiles(MPC_sim,percentiles=percentiles)\n",
    "MPCpercentiles_annual = 1.0 - (1.0 - MPCpercentiles_quarterly)**4\n",
    "\n",
    "print('The MPC at the 10th percentile of the distribution is '+str(decfmt2(MPCpercentiles_annual[0])))\n",
    "print('The MPC at the 50th percentile of the distribution is '+str(decfmt2(MPCpercentiles_annual[4])))\n",
    "print('The MPC at the 90th percentile of the distribution is '+str(decfmt2(MPCpercentiles_annual[-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Very Impatient Households\n",
    "\n",
    "Now that we have some tools for examining both microeconomic (the MPC across the population) and macroeconomic (the distribution and overall level of wealth) outcomes from our model, we are all set to conduct our experiment.\n",
    "\n",
    "In this exercise, we are going to add very impatient households to the economy in a very direct way: by replacing the *most impatient consumer type* with an *even more impatient type*.  Specifically, we will have these agents have a discount factor of $\\beta = 0.80$ at a quarterly frequency, which corresponds to $\\beta \\approx 0.41$ annual.\n",
    "\n",
    "In the code block below, we:\n",
    "\n",
    "1. Replicate the list of agents using $\\texttt{deepcopy}$.\n",
    "2. Set the $\\beta$ of the most impatient type to $0.80$ (for the copied set of agents).\n",
    "3. Solve and simulate the most impatient type (for the copied set of agents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow the instructions above to make another list of agents that includes *very* impatient households.\n",
    "NewTypes = deepcopy(MyTypes)\n",
    "NewTypes[0].DiscFac = 0.8\n",
    "NewTypes[0].solve()\n",
    "NewTypes[0].initializeSim()\n",
    "NewTypes[0].simulate()\n",
    "\n",
    "# Retrieve the MPC's\n",
    "percentiles=np.linspace(0.1,0.9,9)\n",
    "MPC_sim = np.concatenate([ThisType.MPCnow for ThisType in NewTypes])\n",
    "MPCpercentiles_quarterly = getPercentiles(MPC_sim,percentiles=percentiles)\n",
    "MPCpercentiles_annual = 1.0 - (1.0 - MPCpercentiles_quarterly)**4\n",
    "\n",
    "print('The MPC at the 10th percentile of the distribution is '+str(decfmt2(MPCpercentiles_annual[0])))\n",
    "print('The MPC at the 50th percentile of the distribution is '+str(decfmt2(MPCpercentiles_annual[4])))\n",
    "print('The MPC at the 90th percentile of the distribution is '+str(decfmt2(MPCpercentiles_annual[-1])))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "collapsed,code_folding",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
